{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 反向传播和正向传播\n",
    "\n",
    "得到一个batch的前向传播结果后，需要定义一个损失函数来刻画当前的预测值和真实答案之间的误差。然后通过反向传播算法调整神级网络参数的取值，使差距可以被缩小。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from numpy.random import RandomState"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.8113182   1.4845988   0.06532937]\n",
      " [-2.4427042   0.09924842  0.5912244 ]]\n",
      "[[-0.8113182 ]\n",
      " [ 1.4845988 ]\n",
      " [ 0.06532937]]\n",
      "After 0 training step(s),cross entropy on all data is 0.314006\n",
      "After 1000 training step(s),cross entropy on all data is 0.0684551\n",
      "After 2000 training step(s),cross entropy on all data is 0.033715\n",
      "After 3000 training step(s),cross entropy on all data is 0.020558\n",
      "After 4000 training step(s),cross entropy on all data is 0.0136867\n",
      "[[-2.548655   3.0793087  2.8951712]\n",
      " [-4.1112742  1.6259067  3.3972702]]\n",
      "[[-2.3230937]\n",
      " [ 3.3011684]\n",
      " [ 2.4632084]]\n"
     ]
    }
   ],
   "source": [
    "batch_size = 8\n",
    "\n",
    "w1 = tf.Variable(tf.random_normal([2,3],stddev=1,seed=1))\n",
    "w2 = tf.Variable(tf.random_normal([3,1],stddev=1,seed=1))\n",
    "\n",
    "#将数据分成batch\n",
    "x = tf.placeholder(tf.float32,shape=(None,2),name='x-input')\n",
    "y_=tf.placeholder(tf.float32,shape=(None,1),name='y-input')\n",
    "\n",
    "#定义前向传播的过程\n",
    "a = tf.matmul(x,w1)\n",
    "y = tf.matmul(a,w2)\n",
    "\n",
    "#定义损失函数和反向传播的算法\n",
    "y = tf.sigmoid(y)\n",
    "cross_entropy = -tf.reduce_mean(y_*tf.log(tf.clip_by_value(y,1e-10,1.0)) \n",
    "                                + (1-y)*tf.log(tf.clip_by_value(1-y,1e-10,1.0)))\n",
    "train_step = tf.train.AdamOptimizer(0.001).minimize(cross_entropy)\n",
    "#通过随机数生产模拟数据\n",
    "rdm = RandomState(1)\n",
    "dataset_size = 128\n",
    "X= rdm.rand(dataset_size,2)\n",
    "\n",
    "Y = [[int(x1+x2 < 1)] for (x1,x2) in X]\n",
    "\n",
    "#session,训练之前数据\n",
    "with tf.Session() as sess:\n",
    "    init_op =tf.global_variables_initializer()\n",
    "    \n",
    "    sess.run(init_op)\n",
    "    \n",
    "    print(sess.run(w1))\n",
    "    print(sess.run(w2))\n",
    "    \n",
    "    #设定轮数\n",
    "    STEPS = 5000\n",
    "    for i in range(STEPS):\n",
    "        #每次选取batch_size\n",
    "        start = (i* batch_size) % dataset_size\n",
    "        end = min(start+batch_size,dataset_size)\n",
    "        \n",
    "        sess.run(train_step,feed_dict={x:X[start:end],y_:Y[start:end]})\n",
    "        \n",
    "        if i % 1000 == 0:\n",
    "            total_crpss_entropy = sess.run(\n",
    "                cross_entropy,feed_dict={x:X,y_:Y}\n",
    "            )\n",
    "            print ('After %d training step(s),cross entropy on all data is %g'%(i,total_crpss_entropy))\n",
    "            \n",
    "    print (sess.run(w1))\n",
    "    print (sess.run(w2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 步骤\n",
    "1. 定义神经网络的结构和前向传播的输出结果\n",
    "2. 定义损失函数以及选择反向传播的优化算法\n",
    "3. 生成会话，并且在训练集上反复运行反向传播算法。\n",
    "\n",
    "> 无论神经网络的结构如何变化，这三个步骤不会改变"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
