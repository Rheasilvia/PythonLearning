{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 获取MNIST数据集和数据探索"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /home/mj/Documents/data/MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting /home/mj/Documents/data/MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting /home/mj/Documents/data/MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /home/mj/Documents/data/MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets('/home/mj/Documents/data/MNIST_data',one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data size:  55000\n",
      "Validating data size:  5000\n",
      "Testing data size:  10000\n",
      "Example training size:  [0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.02352941 0.43137258 0.9294118  1.\n",
      " 0.7960785  0.0509804  0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.18431373\n",
      " 0.9058824  0.9921569  0.9921569  0.9921569  0.9921569  0.5764706\n",
      " 0.02745098 0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.18823531 0.8980393  0.9921569  0.9921569\n",
      " 0.9921569  0.9921569  0.9921569  0.9921569  0.5647059  0.00392157\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.14509805\n",
      " 0.8941177  0.9921569  0.9921569  0.9921569  0.9921569  0.9921569\n",
      " 0.9921569  0.9921569  0.9921569  0.01960784 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.13725491 0.7019608  0.9921569  0.9921569\n",
      " 0.9921569  0.9921569  0.70980394 0.38823533 0.87843144 0.9921569\n",
      " 0.9921569  0.01960784 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.10196079\n",
      " 0.57254905 0.9921569  0.9921569  0.9921569  0.9450981  0.4431373\n",
      " 0.05490196 0.         0.35686275 0.9843138  0.9921569  0.09411766\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.03137255 0.73333335 0.9921569  0.9921569\n",
      " 0.9921569  0.7176471  0.23137257 0.         0.         0.\n",
      " 0.         0.92549026 0.9921569  0.5058824  0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.03137255\n",
      " 0.5686275  0.9921569  0.9921569  0.9921569  0.7137255  0.07058824\n",
      " 0.         0.         0.         0.         0.         0.92549026\n",
      " 0.9921569  0.5058824  0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.13725491 0.57254905 0.9921569  0.9921569\n",
      " 0.9921569  0.94117653 0.18823531 0.         0.         0.\n",
      " 0.         0.         0.29803923 0.9725491  0.9921569  0.5058824\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.02745098\n",
      " 0.7019608  0.9921569  0.9921569  0.9921569  0.9921569  0.6313726\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.53333336 0.9921569  0.9921569  0.5058824  0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.05882353 0.77647066 0.9921569  0.9921569\n",
      " 0.9921569  0.9921569  0.82745105 0.06666667 0.         0.\n",
      " 0.         0.         0.         0.10588236 0.92549026 0.9921569\n",
      " 0.9921569  0.13725491 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.01568628\n",
      " 0.34509805 0.9921569  0.9921569  0.9490197  0.7490196  0.9921569\n",
      " 0.83921576 0.07843138 0.         0.         0.         0.\n",
      " 0.10980393 0.5647059  0.9921569  0.9921569  0.6313726  0.00784314\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.44705886 0.9921569  0.9921569\n",
      " 0.9921569  0.6901961  0.03529412 0.58431375 0.9921569  0.41960788\n",
      " 0.         0.         0.         0.         0.5176471  0.9921569\n",
      " 0.9921569  0.854902   0.1254902  0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.01176471 0.6901961  0.9921569  0.9921569  0.95294124 0.2392157\n",
      " 0.         0.03529412 0.27450982 0.6117647  0.11764707 0.\n",
      " 0.10980393 0.74509805 0.9686275  0.9921569  0.85098046 0.12941177\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.20392159 0.9921569\n",
      " 0.9921569  0.9921569  0.454902   0.         0.         0.\n",
      " 0.         0.         0.         0.43137258 0.8980393  0.9921569\n",
      " 0.9921569  0.85098046 0.12941177 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.50980395 0.9921569  0.9921569  0.9921569\n",
      " 0.38823533 0.         0.         0.         0.         0.2901961\n",
      " 0.7607844  0.9686275  0.9921569  0.9921569  0.85098046 0.12941177\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.07058824 0.9921569  0.9921569  0.9921569  0.6392157  0.35686275\n",
      " 0.27450982 0.50980395 0.90196085 0.9686275  0.9921569  0.9921569\n",
      " 0.9921569  0.64705884 0.12941177 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.00392157 0.5647059\n",
      " 0.9921569  0.9921569  0.9921569  0.9843138  0.9686275  0.9921569\n",
      " 0.9921569  0.9921569  0.9921569  0.86274517 0.4039216  0.01176471\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.30980393 0.9921569  0.9921569\n",
      " 0.9921569  0.9921569  0.9921569  0.9921569  0.9921569  0.87843144\n",
      " 0.4039216  0.03137255 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.00392157 0.05882353 0.5058824  0.5058824  0.74509805\n",
      " 0.82745105 0.65882355 0.40000004 0.01568628 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.        ]\n",
      "Example training data label: [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "##探索数据\n",
    "print ('Training data size: ',mnist.train.num_examples)\n",
    "\n",
    "print ('Validating data size: ',mnist.validation.num_examples)\n",
    "\n",
    "print ('Testing data size: ',mnist.test.num_examples)\n",
    "\n",
    "print ('Example training size: ',mnist.train.images[0])\n",
    "\n",
    "print ('Example training data label:',mnist.train.labels[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 获取小的batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (100, 784)\n",
      "Y shape: (100, 10)\n"
     ]
    }
   ],
   "source": [
    "batch_size = 100\n",
    "xs,ys = mnist.train.next_batch(batch_size)\n",
    "#从train的集合中获取batch_size大小的训练数据\n",
    "print ('X shape:',xs.shape)\n",
    "print ('Y shape:',ys.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'learning_rate' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-3c311f57c328>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[0;31m#使用梯度下降优化\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m \u001b[0mtrain_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGradientDescentOptimizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mminimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mglobal_step\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mglobal_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;31m#在训练神经网络模型时，每过一遍数据既需要通过反向传播来更新神经网络中的参数。\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'learning_rate' is not defined"
     ]
    }
   ],
   "source": [
    "## 训练模型及不同模型的结果对比\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "#MNIST相关常数\n",
    "INPUT_NODE = 784\n",
    "OUTPUT_NODE = 10\n",
    "\n",
    "#配置神经网络参数\n",
    "LAYER1_NODE = 500 #隐藏层节点数，这里使用一个隐藏层的网络结构数据\n",
    "\n",
    "BATCH_SIZE = 100 #一个训练batch中的训练数据个数，数字越小时，训练过程越接近\n",
    "\n",
    "LEARNING_RATE_BASE = 0.8 #基础的学习率\n",
    "LEARNING_RATE_DECAY = 0.99 #学习率的衰减率\n",
    "\n",
    "REGULARIZATION_RATE = 0.0001 #描述模型复杂度的正则化项在损失函数中的系数\n",
    "TRAINING_STEPS = 300000\n",
    "MOVING_AVERAGE_DECAY=0.99 #滑动平均衰减率\n",
    "\n",
    "def inference(input_tensor,avg_class,weights1,biases1,\n",
    "             weights2,biases2):\n",
    "    #当没有提供滑动平均类时，直接使用参数当前的取值\n",
    "    if avg_class == None:\n",
    "        #计算隐藏层的前向传播结果，这里使用relu激活函数\n",
    "        layer1 = tf.nn.relu(tf.matmul(input_tensor,weights1)+biases1)\n",
    "        #计算输出层的前向传播结果，因为在计算损失函数时候会一并计算softmax函数，\n",
    "        #所以这里不需要加入激活函数，而且不加入softmax不会影响预测结果。因为预测时候\n",
    "        #使用的是不同类别对应节点输出值的相对大小，有没有softmax层对最后分类结果\n",
    "        #计算没有影响，于是在计算整个神经网络的前向传播时可以不加入最后的softmax层\n",
    "        return tf.matmul(layer1,weights2) + biases2\n",
    "    else:\n",
    "        #首先使用avg_class.average函数计算得出变量的滑动平均\n",
    "        layer1 = tf.nn.relu(\n",
    "            tf.matmul(input_tensor,avg_class.average(weights1)) + avg_class.average(biases1)\n",
    "        )\n",
    "        return tf.matmul(layer1,avg_class.average(weights2)) + avg_class.average(biases2)\n",
    "\n",
    "    #训练过程\n",
    "def train(mnist):\n",
    "    x = tf.placeholder(tf.float32,[None,INPUT_NODE],name='x-input')\n",
    "    y_ = tf.placeholder(tf.float32,[None,OUTPUT_NODE],name='y-input')\n",
    "    \n",
    "    #生成隐藏层的参数\n",
    "    weights1 = tf.Variable(tf.truncated_normal([INPUT_NODE,LAYER1_NODE],stddev=0.1))\n",
    "    biass1 = tf.Variable(tf.constant(0.1,shape=[LAYER1_NODE]))\n",
    "    \n",
    "    #生成输出层\n",
    "    weights2 = tf.Variable(tf.truncated_normal([LAYER1_NODE,OUTPUT_NODE],stddev=0.1))\n",
    "    biases2 = tf.Variable(tf.constant(0.1,shape=[OUTPUT_NODE]))\n",
    "    \n",
    "    #前向传播的时候，这里给出的用于计算滑动平均的类为NONE，所以函数不会使用参数的滑动平均\n",
    "    y = inference(x,None,weights1,biass1,weights2,biases2)\n",
    "    \n",
    "    #定义存储训练轮数的变量，这个变量不需要计算滑动平均\n",
    "    #这里指定这歌变量为不可以训练的变量(trainable=False).\n",
    "    #在使用TensorFlow训练神经网络时候\n",
    "    #一般会将代表训练轮数的变量指定为不可训练的参数\n",
    "    global_step = tf.Variable(0,trainable=False)\n",
    "    \n",
    "    #给定滑动平均衰减率和训练轮数的变量，初始化滑动平均类，\n",
    "    #定训练轮数的变量可以加快训练早期变量的更新速度。\n",
    "    variable_averages = tf.train.ExponentialMovingAverage(\n",
    "        MOVING_AVERAGE_DECAY,global_step\n",
    "    )\n",
    "    \n",
    "    #在所有代表神经网络参数的变量上使用滑动平均，其他辅助变量(比如global_step)就不需要了\n",
    "    #获取图上集合的元素\n",
    "    variables_average_op = variable_averages.apply(tf.trainable_variables())\n",
    "    \n",
    "    #计算使用了滑动平均之后的前向传播结果\n",
    "    average_y = inference(x,variable_averages,\n",
    "                         weights1,biass1,weights2,biases2)\n",
    "    #计算交叉熵作为刻画预测值和真实值之间差距的损失函数。\n",
    "    #交叉熵函数，第一个参数不包含softmax层的前向传播结果，第二个是训练数据的正确结果。\n",
    "    cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "        logits=y,labels=tf.arg_max(y_,1)\n",
    "    )\n",
    "    #计算在当前batch中所有样例的交叉熵平均值\n",
    "    cross_entropy_mean = tf.reduce_mean(cross_entropy)\n",
    "    \n",
    "    #计算L2正则化损失\n",
    "    regularizer = tf.contrib.layers.l2_regularizer(REGULARIZATION_RATE)\n",
    "    \n",
    "    #计算模型的正则化损失，一般只计算神经网络边上权重的正则化损失，而不使用偏置项\n",
    "    regularization = regularizer(weights1) + regularizer(weights2)\n",
    "    \n",
    "    #总损失等于交叉熵损失和正则化损失之和\n",
    "    loss = cross_entropy_mean + regularization\n",
    "    \n",
    "    #设置指数衰减的学习率\n",
    "    learning_rate = tf.train.exponential_decay(\n",
    "        LEARNING_RATE_BASE,#基础衰减率\n",
    "        global_step,#当前迭代轮数\n",
    "        mnist.train.num_examples,\n",
    "        LEARNING_RATE_DECAY#学习衰减率\n",
    "    )\n",
    "    \n",
    "#使用梯度下降优化\n",
    "train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss,global_step=global_step)\n",
    "\n",
    "#在训练神经网络模型时，每过一遍数据既需要通过反向传播来更新神经网络中的参数。\n",
    "with tf.control_dependencies([train_step,variables_average_op]):\n",
    "    train_op = tf.no_op(name='train')\n",
    "    \n",
    "#校验使用滑动平均模型的神经网络前向传播结果的正确是否正确\n",
    "correct_prediction = tf.equal(tf.argmax(average_y,1),tf.argmax(y_,1))\n",
    "\n",
    "#这个运算首先将一个布尔型的数值转换为实数，然后计算平均值。这个平均值就是模型在这组数据上的正确性。\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction,tf.float32))\n",
    "\n",
    "#初始化会话并开始训练结果\n",
    "with tf.Session() as sess:\n",
    "    tf.global_variables_initializer().run()\n",
    "    #准备验证数据，一般在神经网络的训练过程中会通过验证数据来大致判断停止的条件和评判训练的效果\n",
    "    validata_feed = {x:mnist.validation.images,\n",
    "                    y_:mnist.validation.labels}\n",
    "    test_feed = {x:mnist.test.images,\n",
    "                y_:mnist.test.labels}\n",
    "    #迭代训练神经\n",
    "    for i in range(TRAINING_STEPS):\n",
    "        if i % 1000 == 0:\n",
    "            validation_acc = sess.run(accuracy,feed_dict=validata_feed)\n",
    "            print('After %d training step(s),validation accuracy'\n",
    "                 'using average model is %g '% (i,validation_acc))\n",
    "            \n",
    "        #产生这一轮使用的一个bacth的训练数据，并运行训练过程\n",
    "        xs,ys = mnist.train.next_batch(BATCH_SIZE)\n",
    "        sess.run(train_op,feed_dict={x:xs,y_:ys})\n",
    "        \n",
    "    #训练集结束后，在测试集上检测神经网络模型的最终正确性\n",
    "    test_acc = sess.run(accuracy,feed_dict=test_feed)\n",
    "    print ('After %d training step(s),test accuracy using average '\n",
    "          'model is %g' % (TRAINING_STEPS,test_acc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /home/mj/Documents/data/MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting /home/mj/Documents/data/MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting /home/mj/Documents/data/MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /home/mj/Documents/data/MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mj/.conda/envs/ten/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2918: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "def main(argv=None):\n",
    "    mnist = input_data.read_data_sets('/home/mj/Documents/data/MNIST_data',one_hot=True)\n",
    "    train(mnist)\n",
    "    \n",
    "if __name__=='__main__':\n",
    "    tf.app.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
